```{r}
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

# Next, we download packages that H2O depends on.
if (! ("methods" %in% rownames(installed.packages()))) { install.packages("methods") }
if (! ("statmod" %in% rownames(installed.packages()))) { install.packages("statmod") }
if (! ("stats" %in% rownames(installed.packages()))) { install.packages("stats") }
if (! ("graphics" %in% rownames(installed.packages()))) { install.packages("graphics") }
if (! ("RCurl" %in% rownames(installed.packages()))) { install.packages("RCurl") }
if (! ("jsonlite" %in% rownames(installed.packages()))) { install.packages("jsonlite") }
if (! ("tools" %in% rownames(installed.packages()))) { install.packages("tools") }
if (! ("utils" %in% rownames(installed.packages()))) { install.packages("utils") }

# Now we download, install and initialize the H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/rel-tverberg/5/R")))
library(h2o)
localH2O = h2o.init(nthreads=-1)
```



```{r}
#install.packages("beepr")
library(beepr)
library(dplyr)
library(lubridate)
codePath = file.path("~/Dropbox","NU","THESIS_MAC","thesis","R CODE")
source(file.path(codePath,"thesis_data_preparation_part3_take2.R"))

modelPath = file.path("~/Dropbox","NU","THESIS_MAC","thesis","MODELS") 
inPath = file.path("~/Dropbox","NU","THESIS_MAC","thesis","DATASETS","GROOMED") # Home path
df = read.csv(file.path(inPath,"greenwich_eltham_imputed_school_hours_take2.csv"),na.strings=c("NA"," "))
 
nodeCount = c(11) #ceiling(colcount/2),5,10) #search on
response        = c('NO2') 

for (nodeNum in 1:length(nodeCount)){
# get data set fields that I can use in the model. 
df.temp.predictors = df %>% select(MMYYYY, Year, Month, Week, Day, TimeHoursNum, BP:WSPD, lag.1.NO:lag.5.WSPD, X5amTo8amSlope.NO2.IMP:min.6.to.8.AM.WSPD.IMP )
includeIdx            = which(names(df) %in% c(response))
dfResponse            = df[,includeIdx]
df = cbind(dfResponse, df.temp.predictors)
colnames(df)[1] = response
#df

# Prep the testing data using 70-20-10 split. End up taking 12.5% for the validation set. This is data that is collected after Jan 2014. 
validationDf    = df %>% filter(Year >= 2014)    # First -- set aside the validation data - i.e. dates/hours after jan 2014
nonValidationDf = df %>% filter(Year  < 2014)  # Second -- divide the remaining df between training and testing using an 80-20 split

# split out the non validation data frame into training and test data
df = nonValidationDf


createDataFrame = function(df,responseCol,params){
  dfParamsInclude   = which(names(df) %in% c(params))
  dfNewParams       = df[,dfParamsInclude]
  dfNewResponseInc  = which(names(df) %in% c(responseCol))
  dfNewResponse     = df[,dfNewResponseInc]
  dfNew             = cbind(dfNewResponse, dfNewParams)
  colnames(dfNew)[1] = paste0(responseCol)
  return(dfNew)
}

chosenVar        = c()
mae.train.latest = c()
mae.test.latest  = c()
mae.xval.latest  = c()

for (i in 1:15){ # number of variables max add = 15! likely forward var select stops at far fewer number of vars however.

##response        = c('O3')    # NO, NOX, SO2, PM10, PM2.5  Done: NO2 (0.5, 5, 10)
results         = c()

# get data set fields that I can use in the model. 
dropIdx               = which(names(df) %in% c('Year', 'MMYYYY', response, chosenVar)) #chosenVar[length(chosenVar)])) #chosenVar))
dfPredictors          = df[,-dropIdx]
predictorVariableList = names(dfPredictors)


# get all values in data frame that are remaining and find all combinations of each --> i changed this to be '1' var added each time so combination no longer needed, but i keep for future ref and change the combination to '1' var.
combn = data.frame(combn(predictorVariableList, 1))

set.seed(123)
smp.size = floor(0.70 * nrow(df))
train = sample(seq_len(nrow(df)), size = smp.size)
test = -train

  if(length(mae.xval.latest)>1){
    if(mae.xval.latest[length(mae.xval.latest)] > mae.xval.latest[length(mae.xval.latest)-1]) {
      break
    }
  }
  
  for (responseVar in 1:1){

    for (j in 1:dim(combn)[2]){  
      
    # create the data frame
    #paramList = as.list(t(combn[j]))    # for this next loop we need to add chosenVar's + 1 from each combn
    #listVars  = chosenVar[length(chosenVar)]
    paramList = append(chosenVar, as.list(t(combn[j])))
    dfNew = createDataFrame(df, response[responseVar], paramList)
     
    # # rename the df to df.train, df.test, and df.vali respectively
    df.train = dfNew[train,]
    df.test  = dfNew[-train,]
    # 
    # # create h2o-ready frame
    df.train.hex <- as.h2o(df.train)
    df.test.hex <- as.h2o(df.test)
     
    # check on colcount
    if(j == 1){
      colcount = 2
    } else {
      colcount = dim(df.train.hex)[2]  
      }
    
    #nodes_layer1 = ceiling(colcount/2)   # 1
    #nodes_layer1 = 5
    #nodes_layer1 = 10
    nodes_layer1 = nodeCount[nodeNum]
    modelID = paste0("colcount.",colcount,".j.",j,"variable.count.",i,".",response,".dl")
    df.train.dl <- h2o.deeplearning(x = 2:colcount, y = 1, training_frame = df.train.hex, validation_frame= df.test.hex, hidden=c(nodes_layer1), 
                                   distribution= "AUTO", variable_importances = TRUE, reproducible = TRUE, seed = 1234, nfolds=10, model_id = modelID) 
  
    mae.train = h2o.mae(df.train.dl, train = TRUE,  valid = FALSE, xval=FALSE)
    mae.test  = h2o.mae(df.train.dl, train = FALSE, valid = TRUE,  xval=FALSE)
    mae.xval  = h2o.mae(df.train.dl, train = FALSE, valid = FALSE, xval=TRUE)

    # next: create the results table which will be a csv file with response, inputVariableName(s), mae train, test, and vali as other columns
    responseCol = response[responseVar]
    inputVarCol = as.character(t(paramList))
  
    results.temp = data.frame(responseCol, j, inputVarCol, mae.train, mae.test, mae.xval)  #
    results = rbind(results, results.temp)

    # save the 'best' model
    model_path  <- paste0("~/Dropbox/NU/THESIS_MAC/thesis/MODELS/",response[1],".dl.","hidden.layers.",nodes_layer1,".colcount.",i,".j.loop.",j) 
    # saved_model <- h2o.loadModel(model_path)    # "/Users/stevenfutter/Dropbox/NU/THESIS/R CODE/~/Dropbox/NU/THESIS/MODELS/no2.dl"
  
    # Save the model
    h2o.saveModel(object=df.train.dl, path=model_path, force=TRUE)  
  
    }

  
  }

# note - had to re-do results since the adding of results to the last results table doesnt work for the 'toString' section.
x = results %>%
  group_by(responseCol, j) %>%
  summarise(predictorVar = toString(inputVarCol)) %>%
  ungroup()

# next: i need to join back to the 'j' col the output for the mae.train, mae.test, and mae.vali
y = inner_join(x, results, by=("j" = "j"))
y = y %>% group_by(j) %>% filter(row_number(mae.train) == 1)
y = y %>% select(j, responseCol.x, predictorVar, mae.train, mae.test, mae.xval)
y = y[,-1]  # remove the 'j'!!

# at this point we have a table of values that we can use, but need to select the one var that minimizes the mae.xval
y = y %>% select(responseCol.x, predictorVar, mae.train, mae.test, mae.xval) %>% slice(which.min(mae.xval))

# variable 1 selected for predicting NO2 is lag.1.NO2. No surprises there. The mae.xval is 4.544425 
# variable 2 selected for predicting NO2 is the lag.1.SOLR.                The mae.xval is 4.360116
# variable 3 selected for predicting NO2 is the lag.2.NO2.                 The mae.xval is 4.259163
addPredictorVar  = unlist(strsplit(y$predictorVar, split=", "))
chosenVar        = append(chosenVar, addPredictorVar[length(addPredictorVar)]) #y$predictorVar)
mae.train.latest = append(mae.train.latest,min(y$mae.train))
mae.test.latest  = append(mae.test.latest,min(y$mae.test))
mae.xval.latest  = append(mae.xval.latest,min(y$mae.xval))

}

# chosenVar
# mae.train.latest
# mae.test.latest
# mae.xval.latest
# nodeCount[nodeNum]




#EXPORT THE MODEL MAE RESULTS TO A CSV
# remove the last value as that has the higher MAE

# 
if (length(chosenVar)==2) {
  chosenVar.final        = chosenVar
  mae.train.latest.final = mae.train.latest
  mae.test.latest.final  = mae.test.latest
  mae.xval.latest.final  = mae.xval.latest
  } else {
  chosenVar.final        = chosenVar[1:length(chosenVar)-1]
  mae.train.latest.final = mae.train.latest[1:length(mae.train.latest)-1]
  mae.test.latest.final  = mae.test.latest[1:length(mae.test.latest)-1]
  mae.xval.latest.final  = mae.xval.latest[1:length(mae.xval.latest)-1]
}

# # Export the data prediction for the testing and validation data sets
outfilePath = file.path("~/Dropbox","NU","THESIS_MAC","thesis","DATASETS","RESULTS", "PREDICTIONS","FORWARD_VAR_PREDICTIONS")
dfExportResults = data.frame(chosenVar.final, mae.train.latest.final, mae.test.latest.final, mae.xval.latest.final, 1)
#dfExportResults

# get min mae.xval predictor inputs
dfExportResults2 = dfExportResults %>%
  group_by(X1) %>%
  summarise(chosenVar.final = toString(chosenVar.final)) %>%
  ungroup()

dfExportResults2 = inner_join(dfExportResults, dfExportResults2, by=("X1" = "X1"))
colnames(dfExportResults2)[length(dfExportResults2)] = 'chosenVar.final'
dfExport = dfExportResults2 %>% select(chosenVar.final, mae.train.latest.final, mae.test.latest.final, mae.xval.latest.final) %>% slice(which.min(mae.xval.latest.final))
dfExport$nodes = as.factor(nodeCount[nodeNum]) #'ceiling(input var count *0.5)')   #,10,5)
dfExport$response = response[1]  # should be O3 now..  #c('NOX') #, 'NO', 'NO')
dfExport$X = 10 # increase +1 each time
dfExport = dfExport %>% select(X, chosenVar.final, mae.train.latest.final, mae.test.latest.final, mae.xval.latest.final, nodes, response)


dfPrior  = read.csv(file.path(outfilePath,"dfExportResultsNO2.csv"),na.strings=c("NA"," "))
dfPrior = dfPrior[2:length(dfPrior)]
colnames(dfExport) = colnames(dfPrior)
dfNew    = rbind(dfPrior, dfExport)

write.csv(dfNew, file.path(outfilePath,'dfExportResultsNO2.csv'))    # # NO2, NO, NOX, O3, PM10, PM2.5, SO2

} 

beep()


# 
# h2o.mae(df.train.dl, train = TRUE, valid = FALSE, xval=FALSE)
# h2o.mae(df.train.dl, train = FALSE, valid = TRUE, xval=FALSE)
# h2o.mae(df.train.dl, train = FALSE, valid = FALSE, xval=TRUE)
# df.train.dl
# 
# chosenVar
# mae.train.latest
# mae.test.latest
# mae.xval.latest
# nodeCount[nodeNum]
# 
# df.train.dl

```




################################################################################################################################################
######### THE MODEL LOADED HERE PROVIDES THE MIN MAE VALUE OF 3.781917 --- it can be loaded from the file path:                          #######
######### "~/Dropbox/NU/THESIS_MAC/thesis/MODELS/NO2.dl.hidden.layers.10.colcount.6.j.loop.21/colcount.7.j.21variable.count.6.NO2.dl"    #######
################################################################################################################################################

```{r}

mae.train  = c()
mae.test   = c()
mae.xval   = c()
model.min  = c()
vars = c()

for (i in 3:3){
  model_path <- paste0("~/Dropbox/NU/THESIS_MAC/thesis/MODELS/NO2.dl.hidden.layers.11.colcount.8.j.loop.",i,"/colcount.9.j.",i,"variable.count.8.NO2.dl")
  saved_model <- h2o.loadModel(model_path)    # "/Users/stevenfutter/Dropbox/NU/THESIS/R CODE/~/Dropbox/NU/THESIS/MODELS/no2.dl"
  
  mae.train.new = h2o.mae(saved_model, train = TRUE,   valid = FALSE, xval=FALSE)
  mae.test.new  = h2o.mae(saved_model, train = FALSE,  valid = TRUE,  xval=FALSE)
  mae.xval.new  = h2o.mae(saved_model, train = FALSE,  valid = FALSE, xval=TRUE)
  
  mae.train = append(mae.train, mae.train.new)
  mae.test  = append(mae.test,  mae.test.new)
  mae.xval  = append(mae.xval,  mae.xval.new)
  
  model.min = append(model.min, i)

}

temp = data.frame(mae.train, mae.test, mae.xval, model.min)
temp = temp %>% slice(which.min(mae.xval))
temp

h2o.varimp(saved_model)[1]


```



# SHOW HOW MAE DECREASES AS WE ADD NODES and introduce complexity to the model
# DRAW PLOTLY OF DAILY PREDICT VS ACTUAL FOR DAYS WHEN OVER 40 ...
```{r}
library(plotly)
f <- list(
  family = "Courier New, monospace",
  size = 18,
  color = "#7f7f7f"
)
x <- list(
  title = "Hour of Day",
  titlefont = f
)
y <- list(
  title = "Average Hourly NO2",
  titlefont = f
)

response       = rep('NO2',10)
hidden.nodes   = seq(2,11)
training.mae   = c(4.821469,4.602355,4.369596,4.396497,3.918334,3.992214,3.758418,3.73644,3.657878, 3.600278)
testing.mae    = c(5.210384,4.961784,4.792736,4.659635,4.2216,4.312087,3.943851,4.090318,4.103943, 3.833438)
validation.mae = c(4.97703,4.745276,4.584833,4.481203,4.075095,4.056564,3.929777,3.872554,3.781917, 3.815724)

#inPathReviewHiddenNodeRes = file.path("~/Dropbox","NU","THESIS_MAC","thesis","DATASETS","RESULTS", "PREDICTIONS","FORWARD_VAR_PREDICTIONS")
#hiddenNodeRes = read.csv(file.path(inPathReviewHiddenNodeRes,"dfExportResultsNO2.csv"),na.strings=c("NA"," "))
hiddenNodeRes = data.frame(response, hidden.nodes, training.mae, testing.mae, validation.mae)
df = hiddenNodeRes %>% filter(response == 'NO2') %>% select(response, hidden.nodes, training.mae, testing.mae, validation.mae)
df = arrange(df, hidden.nodes)
df

p <- plot_ly(df, x = ~hidden.nodes, y=~validation.mae, name = 'Cross-validation MAE', type='scatter', mode='lines+markers') %>%      
  add_trace(y = ~training.mae, name = 'Training MAE',mode = 'lines+markers') %>%
  # add_trace(y = ~euLimit, name = 'Testing MAE', line = list(color = 'rgb(205, 12, 24)', width = 4, dash = 'dash')) %>%
  add_trace(y = ~testing.mae, name = 'Testing MAE', mode = 'line+markers') %>%
  layout(title = "Train, Test, and Validation MAE as Function of Number of Hidden Nodes", 
  xaxis = list(title  = "Number of Hidden Nodes"),
  yaxis = list (title = "MAE (NO2)"))
p
```
