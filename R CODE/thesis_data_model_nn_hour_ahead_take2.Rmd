---
title: "neural network model"
author: "Steven Futter"
date: "5/30/2017"
output: html_document
---

# MODEL 3: ANN using H2O - multinomial classification
http://h2o-release.s3.amazonaws.com/h2o/rel-tverberg/5/index.html
Useful tutorial: https://github.com/h2oai/h2o-tutorials/tree/master/tutorials/deeplearning


ONLY NEED THIS ON FIRST RUN THROUGH
```{r}
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

# Next, we download packages that H2O depends on.
if (! ("methods" %in% rownames(installed.packages()))) { install.packages("methods") }
if (! ("statmod" %in% rownames(installed.packages()))) { install.packages("statmod") }
if (! ("stats" %in% rownames(installed.packages()))) { install.packages("stats") }
if (! ("graphics" %in% rownames(installed.packages()))) { install.packages("graphics") }
if (! ("RCurl" %in% rownames(installed.packages()))) { install.packages("RCurl") }
if (! ("jsonlite" %in% rownames(installed.packages()))) { install.packages("jsonlite") }
if (! ("tools" %in% rownames(installed.packages()))) { install.packages("tools") }
if (! ("utils" %in% rownames(installed.packages()))) { install.packages("utils") }

# Now we download, install and initialize the H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/rel-tverberg/5/R")))
library(h2o)
localH2O = h2o.init(nthreads=-1)
```

# STEP 1: 1-HOUR AHEAD PREDICTIONS
# TRAIN MODEL and CREATE TRAIN, TEST, VALIDATION PREDICTIONS then EXPORT OUTPUT for ALL VARIABLES -- useful for 9AM prediction made at 8AM. 

```{r}
library(dplyr)
library(lubridate)
codePath = file.path("~/Dropbox","NU","THESIS","R CODE")
source(file.path(codePath,"thesis_data_preparation_part3_take2.R"))

inPath = file.path("~/Dropbox","NU","THESIS","DATASETS","GROOMED") # Home path
df = read.csv(file.path(inPath,"greenwich_eltham_imputed_school_hours_take2.csv"),na.strings=c("NA"," "))


# This loop through is needed for prediction hours after 9AM .. i.e. 10am to 4pm
response    = c('NO2', 'NO', 'NOX', 'O3', 'PM10', 'PM2.5', 'SO2', 'BP', 'RAIN', 'RHUM', 'SOLR', 'TEMP', 'WDIR','WSPD')

for (i in 1:1){
#for ( i in 1:length(response)){
createDataFrame = function(df,predictor){
  dfNew = df %>% select(Time, Year, Month, TimeHoursNum,                  # date Time variables
                        lag.1.NO:lag.5.WSPD,                              # lagged meteorological and pollutant variables
                        X5amTo8amSlope.NO2.IMP: X2amTo5amSlope.WSPD.IMP)  # arima slopes-proxy
  dfPred = df[,predictor]
  dfNew = cbind(dfPred, dfNew)
  colnames(dfNew)[1] = paste0(predictor)
  return(dfNew)
}

responseCol = response[i]                 # set first column to the predicted response
df = createDataFrame(df, responseCol)


# Prep the testing data using 70-20-10 split. End up taking 12.5% for the validation set. This is data that is collected after Jan 2014. 
validationDf    = df %>% filter(Year >= 2014)    # First -- set aside the validation data - i.e. dates/hours after jan 2014
nonValidationDf = df %>% filter(Year  < 2014)  # Second -- divide the remaining df between training and testing using an 80-20 split

# split out the non validation data frame into training and test data
df = nonValidationDf

# split out the data 70-30
set.seed(123)
smp.size = floor(0.70 * nrow(df))
train = sample(seq_len(nrow(df)), size = smp.size)
test = -train

# rename the df to df.train, df.test, and df.vali respectively
df.train = df[train,]
df.test  = df[-train,]
df.vali = validationDf


# Train ANN then calculate train / test MSE
########################################################################
# MODEL 1: ANN that uses half as many hidden nodes as variable inputs. 
########################################################################
#library(h2o) # ONLY NEED ON FIRST RUN THROUGH
#h2o.init()   # ONLY NEED ON FIRST RUN THROUGH
df.train.hex <- as.h2o(df.train)
df.test.hex  <- as.h2o(df.test)
df.vali.hex  <- as.h2o(df.vali)

# Rule of thumb that the hidden nodes should be half as many columns (to begin with) -- later we will try other rules here for the number of hidden nodes
colcount    = dim(df.train.hex)[2]
hiddencount = round(dim(df.train.hex)[2]/2)  # need to run the neural network with half as many hidden nodes as input nodes
df.train.dl <- h2o.deeplearning(x = 2:colcount, y = 1, training_frame = df.train.hex, hidden=c(hiddencount), variable_importances = TRUE)
# summary(df.train.dl)

# make a prediction here for train, test, validation set response
predictions.train <- h2o.predict(df.train.dl, df.train.hex)
predictions.test  <- h2o.predict(df.train.dl, df.test.hex)
predictions.vali  <- h2o.predict(df.train.dl, df.vali.hex)

# Export the data prediction for the testing and validation data sets
outfilePath = file.path("~/Dropbox","NU","THESIS","DATASETS","RESULTS", "PREDICTIONS","ONE_HOUR_AHEAD_PREDICTIONS")
filenameTrain = paste0('greenwich_eltham_',response[i],'_ann_train_1_hr_ahead.csv')   # TWO HOURS AHEAD, THREE HOURS, ETC... WILL BE THE NAMES OF THE NEW FILES
filenameTest  = paste0('greenwich_eltham_',response[i],'_ann_test_1_hr_ahead.csv')
filenameVali  = paste0('greenwich_eltham_',response[i],'_ann_vali_1_hr_ahead.csv')

# EXPORT THE TEST DATA SET PREDICTIONS
dfExportTrain = as.data.frame(h2o.cbind(predictions.train,df.train.hex))
dfExportTrain = dfExportTrain %>% filter(TimeHoursNum == '9')
dfExportTrain$TimeKey1 = as.POSIXct(dfExportTrain$Time) + hours(1)
dfExportTrain$TimeKey2 = as.POSIXct(dfExportTrain$Time) + hours(2)
dfExportTrain$TimeKey3 = as.POSIXct(dfExportTrain$Time) + hours(3)
dfExportTrain$TimeKey4 = as.POSIXct(dfExportTrain$Time) + hours(4)
dfExportTrain$TimeKey5 = as.POSIXct(dfExportTrain$Time) + hours(5)
keepIdx = which(names(dfExportTrain) %in% c(response[i], "predict","Time","TimeKey1","TimeKey2","TimeKey3","TimeKey4","TimeKey5"))
dfExportTrain = dfExportTrain[,keepIdx]
write.csv(dfExportTrain, file.path(outfilePath,filenameTrain))   # to here: we have exported a training file containing ALL the 9am predictions for ALL pollutant/meteo vars

# EXPORT THE TEST DATA SET PREDICTIONS
dfExportTest = as.data.frame(h2o.cbind(predictions.test,df.test.hex))
dfExportTest = dfExportTest %>% filter(TimeHoursNum == '9')
dfExportTest$TimeKey1 = as.POSIXct(dfExportTest$Time) + hours(1)
dfExportTest$TimeKey2 = as.POSIXct(dfExportTest$Time) + hours(2)
dfExportTest$TimeKey3 = as.POSIXct(dfExportTest$Time) + hours(3)
dfExportTest$TimeKey4 = as.POSIXct(dfExportTest$Time) + hours(4)
dfExportTest$TimeKey5 = as.POSIXct(dfExportTest$Time) + hours(5)
keepIdx = which(names(dfExportTest) %in% c(response[i], "predict","Time","TimeKey1","TimeKey2","TimeKey3","TimeKey4","TimeKey5"))
dfExportTest = dfExportTest[,keepIdx]
write.csv(dfExportTest, file.path(outfilePath,filenameTest))    # to here: we have exported a testing file containing ALL the 9am predictions for ALL pollutant/meteo vars

# EXPORT THE VALIDATION DATA SET PREDICTIONS
dfExportVali = as.data.frame(h2o.cbind(predictions.vali,df.vali.hex))
dfExportVali = dfExportVali %>% filter(TimeHoursNum == '9')
dfExportVali$TimeKey1 = as.POSIXct(dfExportVali$Time) + hours(1)
dfExportVali$TimeKey2 = as.POSIXct(dfExportVali$Time) + hours(2)
dfExportVali$TimeKey3 = as.POSIXct(dfExportVali$Time) + hours(3)
dfExportVali$TimeKey4 = as.POSIXct(dfExportVali$Time) + hours(4)
dfExportVali$TimeKey5 = as.POSIXct(dfExportVali$Time) + hours(5)
keepIdx = which(names(dfExportVali) %in% c(response[i], "predict","Time","TimeKey1","TimeKey2","TimeKey3","TimeKey4","TimeKey5"))
dfExportVali = dfExportVali[,keepIdx]
write.csv(dfExportVali, file.path(outfilePath,filenameVali))    # to here: we have exported a validation file containing ALL the 9am predictions for ALL pollutant/meteo vars

}
```

# THE 9AM PREDICTION HAS JUST BEEN MADE FOR ALL VARIABLES




# STEP 2 - UPDATE TEST AND VALIDATION FILE WITH "1-HOUR" PREDICTION
# PREDICTED 9AM (TAKEN AT 8AM) BECOMES THE T-1 HOUR LAG FOR THE 10AM PREDICTION. 
# USE TIMEKEY1 FROM THE 9AM PREDICTION IN _ann_test_1_hr_ahead.csv FILE AS THE KEY for the 10AM. At 10am the t-1 lag is 9AM.
# note: IN THE 9AM FILE THE TIMEKEY1 REPRESENTS 10AM KEY, TIMEKEY2 REPRESENTS 11AM KEY... ETC. 

```{r}
# A. RECREATE PRE-PROCESSED TEST RAW DATA PROVIDED -- for TEST and VALIDATION SAMPLES --- REPLACING ACTUAL-LAG-1 WITH PREDICTED-LAG-1
#    LAYOUT NEEDS TO BE EXACTLY THE SAME AS BEFORE.

inPath = file.path("~/Dropbox","NU","THESIS","DATASETS","GROOMED") # Home path
df = read.csv(file.path(inPath,"greenwich_eltham_imputed_school_hours_take2.csv"),na.strings=c("NA"," "))

# This loop through is needed for prediction hours after 9AM .. i.e. 10am to 4pm
response    = c('NO2', 'NO', 'NOX', 'O3', 'PM10', 'PM2.5', 'SO2', 'BP', 'RAIN', 'RHUM', 'SOLR', 'TEMP', 'WDIR','WSPD')

for (i in 1:1){
#for ( i in 1:length(response)){
createDataFrame = function(df,predictor){
  dfNew = df %>% select(Time, Year, Month, TimeHoursNum,                  # date Time variables
                        lag.1.NO:lag.5.WSPD,                              # lagged meteorological and pollutant variables
                        X5amTo8amSlope.NO2.IMP: X2amTo5amSlope.WSPD.IMP)  # arima slopes-proxy
  dfPred = df[,predictor]
  dfNew = cbind(dfPred, dfNew)
  colnames(dfNew)[1] = paste0(predictor)
  return(dfNew)
}

responseCol = response[i]                 # set first column to the predicted response
df = createDataFrame(df, responseCol)

# Prep the testing data using 70-20-10 split. End up taking 12.5% for the validation set. This is data that is collected after Jan 2014. 
validationDf    = df %>% filter(Year >= 2014)    # First -- set aside the validation data - i.e. dates/hours after jan 2014
nonValidationDf = df %>% filter(Year  < 2014)  # Second -- divide the remaining df between training and testing using an 80-20 split

# split out the non validation data frame into training and test data
df = nonValidationDf

# split out the data 70-30
set.seed(123)
smp.size = floor(0.70 * nrow(df))
train = sample(seq_len(nrow(df)), size = smp.size)
test = -train

# rename the df to df.train, df.test, and df.vali respectively
df.train = df[train,]
df.test  = df[-train,]
df.vali = validationDf


}

# names(df.test)
#   [1] "NO2"                     "Time"                    "Year"                    "Month"                   "TimeHoursNum"            "lag.1.NO"               
#   [7] "lag.2.NO"                "lag.3.NO"                "lag.4.NO"                "lag.5.NO"                "lag.1.NO2"               "lag.2.NO2"              
#  [13] "lag.3.NO2"               "lag.4.NO2"               "lag.5.NO2"               "lag.1.NOX"               "lag.2.NOX"               "lag.3.NOX"              
#  [19] "lag.4.NOX"               "lag.5.NOX"               "lag.1.O3"                "lag.2.O3"                "lag.3.O3"                "lag.4.O3"               
#  [25] "lag.5.O3"                "lag.1.PM10"              "lag.2.PM10"              "lag.3.PM10"              "lag.4.PM10"              "lag.5.PM10"             
#  [31] "lag.1.PM2.5"             "lag.2.PM2.5"             "lag.3.PM2.5"             "lag.4.PM2.5"             "lag.5.PM2.5"             "lag.1.SO2"              
#  [37] "lag.2.SO2"               "lag.3.SO2"               "lag.4.SO2"               "lag.5.SO2"               "lag.1.BP"                "lag.2.BP"               
#  [43] "lag.3.BP"                "lag.4.BP"                "lag.5.BP"                "lag.1.RAIN"              "lag.2.RAIN"              "lag.3.RAIN"             
#  [49] "lag.4.RAIN"              "lag.5.RAIN"              "lag.1.RHUM"              "lag.2.RHUM"              "lag.3.RHUM"              "lag.4.RHUM"             
#  [55] "lag.5.RHUM"              "lag.1.SOLR"              "lag.2.SOLR"              "lag.3.SOLR"              "lag.4.SOLR"              "lag.5.SOLR"             
#  [61] "lag.1.TEMP"              "lag.2.TEMP"              "lag.3.TEMP"              "lag.4.TEMP"              "lag.5.TEMP"              "lag.1.WDIR"             
#  [67] "lag.2.WDIR"              "lag.3.WDIR"              "lag.4.WDIR"              "lag.5.WDIR"              "lag.1.WSPD"              "lag.2.WSPD"             
#  [73] "lag.3.WSPD"              "lag.4.WSPD"              "lag.5.WSPD"              "X5amTo8amSlope.NO2.IMP"  "X5amTo8amSlope.NO.IMP"   "X5amTo8amSlope.NOX.IMP" 
#  [79] "X5amTo8amSlope.O3.IMP"   "X5amTo8amSlope.PM10.IMP" "X5amTo8amSlope.SO2.IMP"  "X5amTo8amSlope.BP.IMP"   "X5amTo8amSlope.RAIN.IMP" "X5amTo8amSlope.RHUM.IMP"
#  [85] "X5amTo8amSlope.SOLR.IMP" "X5amTo8amSlope.TEMP.IMP" "X5amTo8amSlope.WDIR.IMP" "X5amTo8amSlope.WSPD.IMP" "X2amTo5amSlope.NO2.IMP"  "X2amTo5amSlope.NO.IMP"  
#  [91] "X2amTo5amSlope.NOX.IMP"  "X2amTo5amSlope.O3.IMP"   "X2amTo5amSlope.PM10.IMP" "X2amTo5amSlope.SO2.IMP"  "X2amTo5amSlope.BP.IMP"   "X2amTo5amSlope.RAIN.IMP"
#  [97] "X2amTo5amSlope.RHUM.IMP" "X2amTo5amSlope.SOLR.IMP" "X2amTo5amSlope.TEMP.IMP" "X2amTo5amSlope.WDIR.IMP" "X2amTo5amSlope.WSPD.IMP"


# B. FUNCTION THAT REPLACES ACTUAL-LAG-1 WITH PREDICTED-LAG-1
updateDF_tMinus1Lags = function(df, lagPredictor, lagPredictorNew, infile){
  inPath  = file.path("~/Dropbox","NU","THESIS","DATASETS","RESULTS", "PREDICTIONS","ONE_HOUR_AHEAD_PREDICTIONS")
  inFile = read.csv(file.path(inPath,infile),na.strings=c("NA"," "))
  inFile = inFile %>% select(predict, TimeKey1)  # note that the TimeKey for the 9AM prdiction will be 10AM... 
  
  inFile$TimeKey1 = as.POSIXct(inFile$TimeKey1)  
  df$Time         = as.POSIXct(df$Time)  
  
  df = full_join(df,inFile,by = c("Time" = "TimeKey1")) 
  df$lagPredictorNew = ifelse(is.na(df$predict),df[,lagPredictorOld],df$predict)
  
  dfNew = df %>% select(everything(), -predict)
  #dropIdx = which(names(df) %in% c(predict, lagPredictorOld))
  #dfNew = df[,-dropIdx]
  colnames(dfNew)[length(names(dfNew))] = lagPredictorNew
  return(dfNew)
}



# C. Next: update each of these function call outs for the old and the new variable names and input files... 
for (i in 1:length(response)){
  lagPredictorOldInputVar = paste0('lag.1.',response[i])
  lagPredictorNewInputVar = paste0('lag.1.',response[i],'.pred')                                                  # no longer needed .new as replacing lag.1 
  filenameInputVar        = paste0('greenwich_eltham_',response[i],'_ann_test_1_hr_ahead.csv')                          # 1 hour ahead is 9AM prediction taken at 8AM
  df.test = updateDF_tMinus1Lags(df.test, lagPredictorOldInputVar, lagPredictorNewInputVar, filenameInputVar) 
}

names(df.test)

# D. DROP COLUMNS NOT USED AND RENAME NEW PREDICTED-LAG-COLUMNS - in prep for predicting 10AM
# drop actual 1-hour lag values
drop.cols = c("lag.1.NO","lag.1.NO2","lag.1.NOX","lag.1.O3","lag.1.PM10","lag.1.PM2.5","lag.1.SO2",
              "lag.1.BP","lag.1.RAIN","lag.1.RHUM","lag.1.SOLR","lag.1.TEMP","lag.1.WDIR","lag.1.WSPD")
df.test = df.test %>% select(-one_of(drop.cols))

# rename predicted to 'actual'... 
names(df.test)[101:114] = drop.cols
names(df.test)
df.test



# NEXT : when i finish this i need to have the exact same format as before


```






















# STEP 3 - PREDICT 2-HOUR AHEAD 
# PREDICTED 9AM (TAKEN AT 8AM) BECOMES THE T-1 HOUR LAG FOR THE 10AM PREDICTION. 
# USE THE df.test DATA FRAME CREATED ABOVE
```{r}
names(df.test)

createDataFrame = function(df,predictor){
  dfNew = df %>% select(Time, Year, Month, TimeHoursNum,                  # date Time variables
                        lag.2.NO:lag.5.WSPD,                              # lagged meteorological and pollutant variables
                        X5amTo8amSlope.NO2.IMP: X2amTo5amSlope.WSPD.IMP,  # arima slopes-proxy
                        lag.1.NO: lag.1.WSPD)                             # predicted 9AM lagged values - these are all that is needed for the 10AM prediction
  dfPred = df[,predictor]
  dfNew = cbind(dfPred, dfNew)
  colnames(dfNew)[1] = paste0(predictor)
  return(dfNew)
}

df.test = createDataFrame(df.test, "NO2")
names(df.test)



str(df.test)
df.test$Time = as.factor(df.test$Time)
#df.train.hex <- as.h2o(df.train)
df.test.hex  <- as.h2o(df.test)         #requires converting Time to factor (POSIX is not recognized)
#df.vali.hex  <- as.h2o(df.vali)

# # Rule of thumb that the hidden nodes should be half as many columns (to begin with) -- later we will try other rules here for the number of hidden nodes
# colcount    = dim(df.train.hex)[2]
# hiddencount = round(dim(df.train.hex)[2]/2)  # need to run the neural network with half as many hidden nodes as input nodes
# df.train.dl <- h2o.deeplearning(x = 2:colcount, y = 1, training_frame = df.train.hex, hidden=c(hiddencount), variable_importances = TRUE)
# # summary(df.train.dl)

# make a prediction here for train, test, validation set response
#predictions.train <- h2o.predict(df.train.dl, df.train.hex)
predictions.test  <- h2o.predict(df.train.dl, df.test.hex)
#predictions.vali  <- h2o.predict(df.train.dl, df.vali.hex)

# Export the data prediction for the testing and validation data sets
outfilePath = file.path("~/Dropbox","NU","THESIS","DATASETS","RESULTS", "PREDICTIONS","ONE_HOUR_AHEAD_PREDICTIONS")
filenameTrain = paste0('greenwich_eltham_',response[i],'_ann_train_2_hr_ahead.csv')   # TWO HOURS AHEAD, THREE HOURS, ETC... WILL BE THE NAMES OF THE NEW FILES
filenameTest  = paste0('greenwich_eltham_',response[i],'_ann_test_2_hr_ahead.csv')
filenameVali  = paste0('greenwich_eltham_',response[i],'_ann_vali_2_hr_ahead.csv')

# # EXPORT THE TEST DATA SET PREDICTIONS
# dfExportTrain = as.data.frame(h2o.cbind(predictions.train,df.train.hex))
# dfExportTrain = dfExportTrain %>% filter(TimeHoursNum == '9')
# dfExportTrain$TimeKey1 = as.POSIXct(dfExportTrain$Time) + hours(1)
# dfExportTrain$TimeKey2 = as.POSIXct(dfExportTrain$Time) + hours(2)
# dfExportTrain$TimeKey3 = as.POSIXct(dfExportTrain$Time) + hours(3)
# dfExportTrain$TimeKey4 = as.POSIXct(dfExportTrain$Time) + hours(4)
# dfExportTrain$TimeKey5 = as.POSIXct(dfExportTrain$Time) + hours(5)
# keepIdx = which(names(dfExportTrain) %in% c(response[i], "predict","Time","TimeKey1","TimeKey2","TimeKey3","TimeKey4","TimeKey5"))
# dfExportTrain = dfExportTrain[,keepIdx]
# write.csv(dfExportTrain, file.path(outfilePath,filenameTrain))   # to here: we have exported a training file containing ALL the 9am predictions for ALL pollutant/meteo vars

# EXPORT THE TEST DATA SET PREDICTIONS
dfExportTest = as.data.frame(h2o.cbind(predictions.test,df.test.hex))
dfExportTest = dfExportTest %>% filter(TimeHoursNum == '10')
dfExportTest
dfExportTest$TimeKey1 = as.POSIXct(dfExportTest$Time) + hours(1)
dfExportTest$TimeKey2 = as.POSIXct(dfExportTest$Time) + hours(2)
dfExportTest$TimeKey3 = as.POSIXct(dfExportTest$Time) + hours(3)
dfExportTest$TimeKey4 = as.POSIXct(dfExportTest$Time) + hours(4)
dfExportTest$TimeKey5 = as.POSIXct(dfExportTest$Time) + hours(5)
keepIdx = which(names(dfExportTest) %in% c(response[i], "predict","Time","TimeKey1","TimeKey2","TimeKey3","TimeKey4","TimeKey5"))
dfExportTest = dfExportTest[,keepIdx]
write.csv(dfExportTest, file.path(outfilePath,filenameTest))    # to here: we have exported a testing file containing ALL the 9am predictions for ALL pollutant/meteo vars

# EXPORT THE VALIDATION DATA SET PREDICTIONS
dfExportVali = as.data.frame(h2o.cbind(predictions.vali,df.vali.hex))
dfExportVali = dfExportVali %>% filter(TimeHoursNum == '9')
dfExportVali$TimeKey1 = as.POSIXct(dfExportVali$Time) + hours(1)
dfExportVali$TimeKey2 = as.POSIXct(dfExportVali$Time) + hours(2)
dfExportVali$TimeKey3 = as.POSIXct(dfExportVali$Time) + hours(3)
dfExportVali$TimeKey4 = as.POSIXct(dfExportVali$Time) + hours(4)
dfExportVali$TimeKey5 = as.POSIXct(dfExportVali$Time) + hours(5)
keepIdx = which(names(dfExportVali) %in% c(response[i], "predict","Time","TimeKey1","TimeKey2","TimeKey3","TimeKey4","TimeKey5"))
dfExportVali = dfExportVali[,keepIdx]
write.csv(dfExportVali, file.path(outfilePath,filenameVali))    # to here: we have exported a validation file containing ALL the 9am predictions for ALL pollutant/meteo vars

}

```



# recreate the test data frame for NO2
```{r}
library(dplyr)
library(lubridate)
codePath = file.path("~/Dropbox","NU","THESIS","R CODE")
source(file.path(codePath,"thesis_data_preparation_part3_take2.R"))

inPath = file.path("~/Dropbox","NU","THESIS","DATASETS","GROOMED") # Home path
df = read.csv(file.path(inPath,"greenwich_eltham_imputed_school_hours_take2.csv"),na.strings=c("NA"," "))

# This loop through is needed for prediction hours after 9AM .. i.e. 10am to 4pm
response    = c('NO2', 'NO', 'NOX', 'O3', 'PM10', 'PM2.5', 'SO2', 'BP', 'RAIN', 'RHUM', 'SOLR', 'TEMP', 'WDIR','WSPD')

for (i in 1:1) {
#for ( i in 1:length(response)){
createDataFrame = function(df,predictor){
  dfNew = df %>% select(Time, Year, Month, TimeHoursNum,                  # date Time variables
                        lag.1.NO:lag.5.WSPD,                              # lagged meteorological and pollutant variables
                        X5amTo8amSlope.NO2.IMP: X2amTo5amSlope.WSPD.IMP)  # arima slopes-proxy
  dfPred = df[,predictor]
  dfNew = cbind(dfPred, dfNew)
  colnames(dfNew)[1] = paste0(predictor)
  return(dfNew)
}

responseCol = response[i]                 # set first column to the predicted response
df = createDataFrame(df, responseCol)


# Prep the testing data using 70-20-10 split. End up taking 12.5% for the validation set. This is data that is collected after Jan 2014. 
validationDf    = df %>% filter(Year >= 2014)    # First -- set aside the validation data - i.e. dates/hours after jan 2014
nonValidationDf = df %>% filter(Year  < 2014)  # Second -- divide the remaining df between training and testing using an 80-20 split

# split out the non validation data frame into training and test data
df = nonValidationDf

# split out the data 70-30
set.seed(123)
smp.size = floor(0.70 * nrow(df))
train = sample(seq_len(nrow(df)), size = smp.size)
test = -train

# rename the df to df.train, df.test, and df.vali respectively
df.test  = df[-train,]
df.vali = validationDf
df.test.hex  <- as.h2o(df.test)
df.vali.hex  <- as.h2o(df.vali)

df.test
}

```